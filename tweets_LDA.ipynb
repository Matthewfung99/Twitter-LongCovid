{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PACKAGES","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport gensim\nimport re\nimport nltk\nfrom tqdm import tqdm\n\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom gensim import models\nfrom gensim.models.coherencemodel import CoherenceModel\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-25T14:04:42.098281Z","iopub.execute_input":"2023-03-25T14:04:42.098736Z","iopub.status.idle":"2023-03-25T14:04:42.116567Z","shell.execute_reply.started":"2023-03-25T14:04:42.098696Z","shell.execute_reply":"2023-03-25T14:04:42.114982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FUNCTION","metadata":{}},{"cell_type":"code","source":"def tweet_density(tweets_df):\n    tweets_df.clean_tweet.fillna(' ', inplace = True)\n    tweets_df['clean_tweet']=tweets_df.clean_tweet.str.replace('long covid',' ')\n    tweets_df['clean_tweet']=tweets_df.clean_tweet.str.replace('covid',' ')\n    tweets_df['clean_tweet']=tweets_df.clean_tweet.str.replace('covid',' ')\n    # Convert date to datetime\n    tweets_df['date'] = pd.to_datetime(tweets_df['Date'], errors='coerce')\n    # Add column for year\n    tweets_df['year'] = tweets_df['date'].dt.year\n    tweets_df['month'] = tweets_df['date'].dt.month\n    tweets_df['day'] = tweets_df['date'].dt.day\n    daily_counts = tweets_df.set_index('date').resample('D').count()\n    daily_counts.plot(figsize=(16,10))\n    plt.title('Daily Counts of Tweets')\n    plt.plot(daily_counts);\n    plt.show()\n    return tweets_df\n\n\ndef vectorize_dic_bag(tweets_df):\n    tweets_tokens = tweets_df.clean_tweet.apply(lambda x: re.split('\\s', str(x)))\n    dictionary = gensim.corpora.Dictionary(tweets_tokens)\n    bow_corpus = [dictionary.doc2bow(tweet) for tweet in tweets_tokens]\n    return tweets_tokens,dictionary,bow_corpus\n\n\ndef u_mass(dictionary):\n    filtered_dict=dictionary\n    coherenceList_umass = []\n    num_topics_list = np.arange(7,23)\n    min=99999\n    num=0\n    itera=7\n    for num_topics in tqdm(num_topics_list):\n        lda = models.LdaMulticore(corpus=bow_corpus, num_topics=num_topics, id2word=dictionary, \n                                  passes=10,chunksize=4000,workers=None,random_state=0)\n        cm = CoherenceModel(model=lda, corpus=bow_corpus, \n                            dictionary=filtered_dict, coherence='u_mass')\n        coherenceList_umass.append(cm.get_coherence())\n        if cm.get_coherence()<=min:\n            min=cm.get_coherence()\n            num=itera\n        itera+=1\n        #viz = pyLDAvis.gensim.prepare(lda, bow_corpus, filtered_dict, mds='tsne')\n        #pyLDAvis.save_html(viz,f'pyLDAvis_{num_topics}.html')\n    return filtered_dict, coherenceList_umass, num_topics_list, num, lda, cm\n\n\ndef plot_TC(num_topics_list,coherenceList_umass):\n    plotData = pd.DataFrame({'Number of topics':num_topics_list,\n                             'CoherenceScore':coherenceList_umass})\n    f,ax = plt.subplots(figsize=(30,10))\n    sns.set_style(\"darkgrid\")\n    sns.set(font_scale = 2)\n    sns.pointplot(x='Number of topics', y= 'CoherenceScore',data=plotData)\n    plt.axhline(y=-4.8, color='red')\n    plt.title('Topic Coherence')\n    #plt.savefig('/kaggle/working/topic_coherence.png')\n    return plotData\n\n\ndef get_matrix(tweets_df):\n    tweets_df.clean_tweet.fillna(' ', inplace = True)\n    tweets_df['clean_tweet']=tweets_df.clean_tweet.str.replace('long covid',' ')\n    tweets_df['clean_tweet']=tweets_df.clean_tweet.str.replace('covid',' ')\n    count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n    tweet_text = tweets_df['clean_tweet'].values\n    document_term_matrix = count_vectorizer.fit_transform(tweet_text)\n    return count_vectorizer,document_term_matrix\n\n\ndef topic_fitting(num,document_term_matrix):\n    n_topics = num\n    lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', batch_size=10000, \n                                              random_state=0, learning_decay=0.5, verbose=0)\n    lda_topic_matrix = lda_model.fit_transform(document_term_matrix)\n    return n_topics,lda_topic_matrix\n\n\ndef get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)\n\n\ndef get_top_n_words(n, keys, document_term_matrix, count_vectorizer,n_topics):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        #temp_vector_sum = temp_vector_sum.toarray()\n        \n        #add to deal with the error；int don't have toarray()\n        try:\n            if(temp_vector_sum.toarray):\n                previous = temp_vector_sum\n                #print(temp_vector_sum.toarray(),'\\n')\n                temp_vector_sum = temp_vector_sum.toarray()\n        except(AttributeError):\n            print('topic'+str(topic)+'error!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n            temp_vector_sum = previous.toarray()\n        \n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words\n\n\ndef save_topic(lda_keys,document_term_matrix,count_vectorizer,sheet_name):\n    top_n_words_lda = get_top_n_words(15, lda_keys, document_term_matrix, count_vectorizer,n_topics)\n    topic_list=[]\n    for i in range(len(top_n_words_lda)):\n        print(\"Topic {}: \".format(i+1), top_n_words_lda[i])\n        topic_list.append(top_n_words_lda[i])\n    data = {sheet_name:topic_list}\n    #LDA_topic = pd.DataFrame(data)\n    #LDA_topic.to_csv(sheet_name+'.csv')\n    return data\n\n\ndef plot_topic(lda_keys,document_term_matrix,count_vectorizer,n_topics,plot_name):\n    top_5_words = get_top_n_words(15, lda_keys, document_term_matrix, count_vectorizer,n_topics) \n    labels = ['Topic {}: \\n'.format(i) + top_5_words[i] for i in lda_categories]\n    fig, ax = plt.subplots(figsize=(120,20))\n    content = [top_5_words[i] for i in lda_categories]\n    data={'topic_index':lda_categories,'topic_content':content,'tweets_num':lda_counts}\n    LDA_topic = pd.DataFrame(data)\n    sort_topic = LDA_topic.sort_values(by='topic_index',ascending=True)\n    sort_topic = sort_topic.reset_index(drop=True)\n    sort_topic.to_csv(plot_name+'.csv')\n    print('\\n')\n    print(sort_topic)\n    print('\\n')\n    #LDA_topic.to_csv(plot_name+'.csv')\n    ax.bar(lda_categories, lda_counts);\n    ax.set_xticks(lda_categories);\n    ax.set_xticklabels(labels);\n    ax.set_title('LDA topic counts');\n    ax.set_ylabel('Number of tweets');\n    #plt.savefig('/kaggle/working/LDA_topic'+plot_name+'.png')\n    return top_5_words,labels","metadata":{"execution":{"iopub.status.busy":"2023-03-25T15:18:46.666773Z","iopub.execute_input":"2023-03-25T15:18:46.667368Z","iopub.status.idle":"2023-03-25T15:18:46.707237Z","shell.execute_reply.started":"2023-03-25T15:18:46.667316Z","shell.execute_reply":"2023-03-25T15:18:46.706014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONDUCT","metadata":{}},{"cell_type":"code","source":"%%time\n\n# tweets_df_2020_p = pd.read_csv('/kaggle/input/tweets/PCS_patient_2020_clean.csv')\n# tweets_df_2021_p = pd.read_csv('/kaggle/input/tweets/PCS_patient_2021_clean.csv')\n# tweets_df_2022_p = pd.read_csv('/kaggle/input/tweets/PCS_patient_2022_clean.csv')\n# tweets_df_2023_p = pd.read_csv('/kaggle/input/tweets/PCS_patient_2023_clean.csv')\n\n##########################################\n#create file name\nfilelist=[]\nfor i in range(4):\n        filelist.append('PCS_non_patient_202'+str(i)+'_clean.csv')\n# print(filelist)\n# ['PCS_patient_2020_clean.csv', 'PCS_patient_2021_clean.csv', 'PCS_patient_2022_clean.csv', 'PCS_patient_2023_clean.csv', \n#  'PCS_non_patient_2020_clean.csv', 'PCS_non_patient_2021_clean.csv', 'PCS_non_patient_2022_clean.csv', 'PCS_non_patient_2023_clean.csv']\ncreateVar = locals()\nmyVarList = [] # 存放自己创建的变量\nfor i in range(len(filelist)):\n    createVar[filelist[i]] = pd.read_csv('/kaggle/input/tweets/'+filelist[i])\n    myVarList.append(createVar[filelist[i]])\n    if(i<2):\n        continue\n#     #to get the remaining data\n#     #-------------------------------------------------------------\n#     if i < 6:\n#         continue\n#     #-------------------------------------------------------------\n    for j in range(12):\n        if (i==2 and j+1<=11):\n            continue\n        if myVarList[i][myVarList[i]['month'].isin([j+1])].empty:\n            continue\n        \n#         #to get the remaining data\n#         #-------------------------------------------------------------\n#         if i == 6 and j < 7:\n#             continue\n#         #-------------------------------------------------------------\n        \n        else:\n            tweets_df=myVarList[i][myVarList[i]['month'].isin([j+1])]\n            tweets_df = tweet_density(tweets_df)\n\n            tweets_tokens,dictionary,bow_corpus = vectorize_dic_bag(tweets_df)\n            \n            filtered_dict,coherenceList_umass,num_topics_list,num,lda,cm = u_mass(dictionary)\n\n            plotData=plot_TC(num_topics_list,coherenceList_umass)\n\n            lda_model_bow = gensim.models.LdaMulticore(corpus=bow_corpus, num_topics=num, id2word=dictionary, decay=0.5,\n                                                       chunksize=10000, passes=10, workers=None, random_state=0)\n            lda_viz = pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary, mds='tsne')\n            pyLDAvis.enable_notebook()\n            lda_viz\n\n            count_vectorizer,document_term_matrix=get_matrix(tweets_df)\n            n_topics,lda_topic_matrix=topic_fitting(num,document_term_matrix)\n            lda_keys = get_keys(lda_topic_matrix)\n            lda_categories, lda_counts = keys_to_counts(lda_keys)\n#             try:\n#                 get_top_n_words(n, keys, document_term_matrix, count_vectorizer,n_topics)\n                \n            if i<4:\n                #LDA_topic=save_topic(lda_keys,document_term_matrix,count_vectorizer,'topic_p_202'+str(i)+'_'+str(j+1))\n                #LDA_topic\n                top_5_words,labels=plot_topic(lda_keys,document_term_matrix,count_vectorizer,n_topics,'topic_np_202'+str(i)+'_'+str(j+1))\n##########################################\n\n\n\n#--------------------------------------------------------------------------------------------------------------------------------------\n# tweets_df_2020_p = pd.read_csv('/kaggle/input/tweets/PCS_patient_2020_clean.csv')\n# tweets_df=tweets_df_2020_p\n# tweets_df=tweets_df[tweets_df['month'].isin([5])]\n# tweets_df = tweet_density(tweets_df)\n\n# tweets_tokens,dictionary,bow_corpus = vectorize_dic_bag(tweets_df)\n\n# filtered_dict,coherenceList_umass,num_topics_list,num,lda,cm = u_mass(dictionary)\n\n# plotData=plot_TC(num_topics_list,coherenceList_umass)\n\n# lda_model_bow = gensim.models.LdaMulticore(corpus=bow_corpus, num_topics=num, id2word=dictionary, decay=0.5,\n#                                            chunksize=10000, passes=10, workers=None, random_state=0)\n# lda_viz = pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary, mds='tsne')\n# pyLDAvis.enable_notebook()\n# lda_viz\n\n# count_vectorizer,document_term_matrix=get_matrix(tweets_df)\n# n_topics,lda_topic_matrix=topic_fitting(num,document_term_matrix)\n# lda_keys = get_keys(lda_topic_matrix)\n# lda_categories, lda_counts = keys_to_counts(lda_keys)\n# # LDA_topic=save_topic(lda_keys,document_term_matrix,count_vectorizer,sheet_name)\n# #sheet_name need to be changed\n# LDA_topic=save_topic(lda_keys,document_term_matrix,count_vectorizer,'topic_np_2020_5')\n# LDA_topic\n\n# top_5_words,labels=plot_topic(lda_keys,document_term_matrix,count_vectorizer,n_topics,'_2020_5')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-25T15:18:48.076107Z","iopub.execute_input":"2023-03-25T15:18:48.076828Z","iopub.status.idle":"2023-03-25T15:19:00.626181Z","shell.execute_reply.started":"2023-03-25T15:18:48.076772Z","shell.execute_reply":"2023-03-25T15:19:00.624944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}